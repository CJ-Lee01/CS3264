{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48834c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d6161fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6d8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "    DB_NAME = \"fake_news_db\"\n",
    "    COLLECTION_NAME = \"news_embeddings\"\n",
    "    EMBEDDING_MODEL = \"distilbert-base-uncased\"  # Lightweight model for embeddings\n",
    "    LLM_MODEL = \"distilbert-base-uncased\"  # The main Gemma model\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    MAX_LENGTH = 2048  # Maximum token length for the model\n",
    "    # File paths\n",
    "    FAKE_NEWS_FILE = \"fake.csv\"\n",
    "    TRUE_NEWS_FILE = \"true.csv\"\n",
    "    FINE_FAKE_FILE = \"FineFake.csv\"  \n",
    "\n",
    "config = Config()\n",
    "print(f\"Using device: {config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36e102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data():\n",
    "    \"\"\"\n",
    "    Load and combine fake and true news datasets for training\n",
    "    \"\"\"\n",
    "    # Load fake news\n",
    "    fake_df = pd.read_csv(config.FAKE_NEWS_FILE, sep='\\t')\n",
    "    fake_df['label'] = 1  # 1 for fake news\n",
    "    \n",
    "    # Load true news\n",
    "    true_df = pd.read_csv(config.TRUE_NEWS_FILE)\n",
    "    true_df['label'] = 0  # 0 for true news\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "    combined_df = combined_df.dropna(subset=['text'])\n",
    "    \n",
    "    # Create content field combining title and text for better context\n",
    "    combined_df['content'] = combined_df['title'].fillna('') + \" \" + combined_df['text'].fillna('')\n",
    "    \n",
    "    print(f\"Loaded {len(combined_df)} news articles ({len(fake_df)} fake, {len(true_df)} true)\")\n",
    "    return combined_df\n",
    "\n",
    "def load_fine_grained_data():\n",
    "    \"\"\"\n",
    "    Load the detailed metadata file for MongoDB storage\n",
    "    This file has rich features like knowledge embeddings, entities, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to load the file - first checking if it's JSON\n",
    "        try:\n",
    "            df = pd.read_json(config.FINE_FAKE_FILE, lines=True)\n",
    "        except:\n",
    "            # If not JSON, try CSV with tab separator\n",
    "            df = pd.read_csv(config.FINE_FAKE_FILE, sep='\\t')\n",
    "        \n",
    "        # Process the knowledge embeddings if they exist\n",
    "        if 'knowledge_embedding' in df.columns:\n",
    "            # Convert string representation of embeddings to actual arrays\n",
    "            df['knowledge_embedding'] = df['knowledge_embedding'].apply(\n",
    "                lambda x: np.array(ast.literal_eval(x)) if isinstance(x, str) else x\n",
    "            )\n",
    "        \n",
    "        print(f\"Loaded {len(df)} detailed news articles for database\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fine-grained data: {e}\")\n",
    "        print(\"Will continue without detailed metadata.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25271b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Remove extra quotes if present\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1]\n",
    "        \n",
    "    # Basic preprocessing\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf41240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a9d4acf5044488961303d9525df4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "token = \"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\"\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaed5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_pipeline(model):\n",
    "    \"\"\"Create a text generation pipeline using just the model\"\"\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        max_length=config.MAX_LENGTH,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return pipe\n",
    "\n",
    "def setup_mongodb():\n",
    "    \"\"\"Connect to MongoDB and set up vector collections\"\"\"\n",
    "    client = MongoClient(config.MONGODB_URI)\n",
    "    db = client[config.DB_NAME]\n",
    "    collection = db[config.COLLECTION_NAME]\n",
    "    \n",
    "    # Create vector search index if it doesn't exist\n",
    "    try:\n",
    "        if \"vector_index\" not in collection.index_information():\n",
    "            # For MongoDB Atlas or versions supporting vector search\n",
    "            collection.create_index([(\"embedding\", \"vector\")])\n",
    "            print(\"Created vector search index\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Vector index creation failed: {e}\")\n",
    "        print(\"This may be expected if not using MongoDB Atlas or a version with vector search support\")\n",
    "        print(\"The system will still store embeddings but vector search capabilities may be limited\")\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9f8e455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and MongoDB collection initialized!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    # Initialize the classification pipeline\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "    print(\"Models and MongoDB collection initialized!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during initialization: {e}\")\n",
    "    print(\"Try running with minimal resources or in separate steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40e1be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.errors import AutoReconnect, NetworkTimeout, ConnectionFailure\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=4, max=60))\n",
    "def mongodb_operation(collection, operation, *args, **kwargs):\n",
    "    \"\"\"Wrapper for MongoDB operations with retry logic\"\"\"\n",
    "    try:\n",
    "        if operation == \"update_one\":\n",
    "            return collection.update_one(*args, **kwargs)\n",
    "        elif operation == \"find_one\":\n",
    "            return collection.find_one(*args, **kwargs)\n",
    "        # Add other operations as needed\n",
    "    except (AutoReconnect, NetworkTimeout, ConnectionFailure) as e:\n",
    "        print(f\"MongoDB connection error: {e}. Retrying...\")\n",
    "        # Recreate client connection before retry\n",
    "        raise  # This will trigger the retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56aa572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_store_embeddings(df, embedding_model, mongodb_uri, db_name, collection_name, batch_size=50):\n",
    "    \"\"\"Generate embeddings for news articles and store in MongoDB with robust error handling\"\"\"\n",
    "    print(f\"Generating embeddings for {len(df)} documents...\")\n",
    "    \n",
    "    # Process in smaller batches to avoid memory issues and timeout risks\n",
    "    successful_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(df) + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "        # Create a fresh connection for each batch\n",
    "        try:\n",
    "            client = MongoClient(mongodb_uri, serverSelectionTimeoutMS=30000)\n",
    "            db = client[db_name]\n",
    "            collection = db[collection_name]\n",
    "            \n",
    "            # Test connection before proceeding\n",
    "            client.admin.command('ping')\n",
    "            print(\"Connected to MongoDB successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to MongoDB: {e}\")\n",
    "            time.sleep(10)  # Wait before trying to reconnect\n",
    "            continue\n",
    "            \n",
    "        for _, row in batch_df.iterrows():\n",
    "            try:\n",
    "                # Create base document with universally expected fields\n",
    "                doc = {\n",
    "                    \"id\": str(row.name),\n",
    "                    \"content_type\": \"news_article\"\n",
    "                }\n",
    "                \n",
    "                # Add all available fields from the dataframe\n",
    "                for col in row.index:\n",
    "                    if col != 'content' and col != 'knowledge_embedding' and not pd.isna(row[col]):\n",
    "                        # Skip content (we'll process it separately) and knowledge_embedding\n",
    "                        doc[col] = row[col]\n",
    "                \n",
    "                # Prepare text for embedding\n",
    "                if 'content' in row and not pd.isna(row['content']):\n",
    "                    content = row['content']\n",
    "                elif 'text' in row and not pd.isna(row['text']):\n",
    "                    if 'title' in row and not pd.isna(row['title']):\n",
    "                        content = f\"{row['title']} {row['text']}\"\n",
    "                    else:\n",
    "                        content = row['text']\n",
    "                else:\n",
    "                    # Skip this document if no content to embed\n",
    "                    print(f\"Skipping row {row.name}: No content to embed\")\n",
    "                    continue\n",
    "                    \n",
    "                # Process the text\n",
    "                processed_text = preprocess_text(content)\n",
    "                \n",
    "                # Store the original and processed content\n",
    "                doc['original_content'] = content\n",
    "                doc['processed_content'] = processed_text\n",
    "                \n",
    "                # Use pre-computed knowledge embedding if available\n",
    "                if 'knowledge_embedding' in row and not pd.isna(row['knowledge_embedding']):\n",
    "                    if isinstance(row['knowledge_embedding'], np.ndarray):\n",
    "                        doc['embedding'] = row['knowledge_embedding'].tolist()\n",
    "                    else:\n",
    "                        # Try to parse the embedding if it's a string\n",
    "                        try:\n",
    "                            doc['embedding'] = ast.literal_eval(row['knowledge_embedding'])\n",
    "                        except:\n",
    "                            # If parsing fails, generate a new embedding\n",
    "                            doc['embedding'] = embedding_model.encode(processed_text).tolist()\n",
    "                else:\n",
    "                    # Generate new embedding\n",
    "                    doc['embedding'] = embedding_model.encode(processed_text).tolist()\n",
    "                \n",
    "                # Store in MongoDB with retry logic\n",
    "                mongodb_operation(collection, \"update_one\", {\"id\": doc[\"id\"]}, {\"$set\": doc}, upsert=True)\n",
    "                successful_count += 1\n",
    "                \n",
    "                # Periodically log progress\n",
    "                if successful_count % 10 == 0:\n",
    "                    print(f\"Successfully processed {successful_count} documents\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"Error processing row {row.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Close connection after each batch\n",
    "        client.close()\n",
    "        print(f\"Completed batch {i//batch_size + 1}\")\n",
    "        \n",
    "        # Brief pause between batches to avoid overloading the connection\n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(f\"Embedding generation complete! Successfully processed {successful_count} documents. Errors: {error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84aa4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Splitting Functions\n",
    "def load_and_split_data():\n",
    "    \"\"\"\n",
    "    Load and split data from all sources with an 80-20 train-test split\n",
    "    \"\"\"\n",
    "    data_sources = []\n",
    "    \n",
    "    # Load fake news\n",
    "    try:\n",
    "        fake_df = pd.read_csv(config.FAKE_NEWS_FILE, sep='\\t')\n",
    "        fake_df['label'] = 1  # 1 for fake news\n",
    "        fake_df['source'] = 'fake.csv'\n",
    "        data_sources.append(fake_df)\n",
    "        print(f\"Loaded {len(fake_df)} articles from fake.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fake.csv: {e}\")\n",
    "    \n",
    "    # Load true news\n",
    "    try:\n",
    "        true_df = pd.read_csv(config.TRUE_NEWS_FILE)\n",
    "        true_df['label'] = 0  # 0 for true news\n",
    "        true_df['source'] = 'true.csv'\n",
    "        data_sources.append(true_df)\n",
    "        print(f\"Loaded {len(true_df)} articles from true.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading true.csv: {e}\")\n",
    "    \n",
    "    # Load fine-grained data\n",
    "    try:\n",
    "        # Try to load the file - first checking if it's JSON\n",
    "        try:\n",
    "            fine_df = pd.read_json(config.FINE_FAKE_FILE, lines=True)\n",
    "        except:\n",
    "            # If not JSON, try CSV with tab separator\n",
    "            fine_df = pd.read_csv(config.FINE_FAKE_FILE, sep='\\t')\n",
    "        \n",
    "        # Process the knowledge embeddings if they exist\n",
    "        if 'knowledge_embedding' in fine_df.columns:\n",
    "            fine_df['knowledge_embedding'] = fine_df['knowledge_embedding'].apply(\n",
    "                lambda x: np.array(ast.literal_eval(x)) if isinstance(x, str) else x\n",
    "            )\n",
    "        \n",
    "        # Ensure label exists\n",
    "        if 'label' not in fine_df.columns:\n",
    "            fine_df['label'] = 1  # Assume fake if not specified\n",
    "            \n",
    "        fine_df['source'] = 'finefake'\n",
    "        data_sources.append(fine_df)\n",
    "        print(f\"Loaded {len(fine_df)} articles from finefake\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading finefake: {e}\")\n",
    "    \n",
    "    # Combine all data sources\n",
    "    all_data = pd.concat(data_sources, ignore_index=True)\n",
    "    all_data = all_data.dropna(subset=['text'])\n",
    "    \n",
    "    # Create content field combining title and text for better context\n",
    "    all_data['content'] = all_data['title'].fillna('') + \" \" + all_data['text'].fillna('')\n",
    "    \n",
    "    # Shuffle the data\n",
    "    all_data = all_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split into train (80%) and test (20%) sets\n",
    "    split_idx = int(len(all_data) * 0.8)\n",
    "    train_df = all_data.iloc[:split_idx]\n",
    "    test_df = all_data.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Split data into {len(train_df)} training and {len(test_df)} test samples\")\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a8e5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def retrieve_similar_documents(query: str, k: int = 5) -> list[str]:\n",
    "    model = SentenceTransformer(config.EMBEDDING_MODEL)\n",
    "    query_embedding = model.encode(query)\n",
    "\n",
    "    client = MongoClient(config.MONGODB_URI)\n",
    "    collection = client[config.DB_NAME][config.COLLECTION_NAME]\n",
    "\n",
    "    docs = collection.find()\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        doc_embedding = np.array(doc['embedding'])\n",
    "        score = cosine_similarity(query_embedding, doc_embedding)\n",
    "        results.append((score, doc['content']))\n",
    "\n",
    "    results.sort(reverse=True)\n",
    "    return [content for _, content in results[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a85c955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_claim(claim: str) -> str:\n",
    "    evidence = retrieve_similar_documents(claim)\n",
    "    context = \"\\n\".join(evidence)\n",
    "    \n",
    "    prompt = f\"\"\"You are a fact-checking assistant. Based on the following evidence, determine whether the claim is REAL or FAKE.\n",
    "\n",
    "Evidence:\n",
    "{context}\n",
    "\n",
    "Claim: {claim}\n",
    "\n",
    "Label (REAL or FAKE):\"\"\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.LLM_MODEL)\n",
    "    model = AutoModelForCausalLM.from_pretrained(config.LLM_MODEL).to(config.DEVICE)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=config.MAX_LENGTH).to(config.DEVICE)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return prediction.split(\":\")[-1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd566be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_mongodb_collection(collection_name):\n",
    "    \"\"\"Create a MongoDB collection with proper indexes\"\"\"\n",
    "    client = MongoClient(config.MONGODB_URI)\n",
    "    db = client[config.DB_NAME]\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    # Create vector search index if it doesn't exist\n",
    "    try:\n",
    "        if \"vector_index\" not in collection.index_information():\n",
    "            collection.create_index([(\"embedding\", \"vector\")])\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Vector index creation failed: {e}\")\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65bf2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def process_and_save_data(embedding_model):\n",
    "    \"\"\"Process data and save to disk or load from cache\"\"\"\n",
    "    # File paths for saved data\n",
    "    train_path = \"processed_data/train_embeddings.pkl\"\n",
    "    test_path = \"processed_data/test_embeddings.pkl\"\n",
    "\n",
    "    os.makedirs(\"processed_data\", exist_ok=True)\n",
    "\n",
    "    # Load from cache if available\n",
    "    if os.path.exists(train_path) and os.path.exists(test_path):\n",
    "        print(\"Loading processed data from disk...\")\n",
    "        with open(train_path, 'rb') as f:\n",
    "            train_data = pickle.load(f)\n",
    "        with open(test_path, 'rb') as f:\n",
    "            test_data = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Processing data from scratch...\")\n",
    "        train_df, test_df = load_and_split_data()\n",
    "\n",
    "        train_data = generate_docs_with_embeddings(train_df, embedding_model, tokenizer)\n",
    "        test_data = generate_docs_with_embeddings(test_df, embedding_model, tokenizer)\n",
    "\n",
    "        with open(train_path, 'wb') as f:\n",
    "            pickle.dump(train_data, f)\n",
    "        with open(test_path, 'wb') as f:\n",
    "            pickle.dump(test_data, f)\n",
    "\n",
    "    # Store in MongoDB\n",
    "    store_in_mongodb(train_data, config.MONGODB_URI, config.DB_NAME, config.COLLECTION_NAME)\n",
    "    store_in_mongodb(test_data, config.MONGODB_URI, config.DB_NAME, config.COLLECTION_NAME + \"_test\")\n",
    "\n",
    "    return len(train_data), len(test_data)\n",
    "\n",
    "\n",
    "def generate_docs_with_embeddings(df, embedding_model, tokenizer, batch_size=16):\n",
    "    \"\"\"Create embedded documents from a dataframe using DistilBERT for embeddings\"\"\"\n",
    "    docs = []\n",
    "    batch_texts = []\n",
    "    batch_idx = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 100 == 0:  # Print progress every 100 samples\n",
    "            print(f\"Processing document {idx}...\")\n",
    "        # Compose content text\n",
    "        title = str(row.get('title', '') or '')\n",
    "        text = str(row.get('text', '') or '')\n",
    "        content = f\"{title.strip()} {text.strip()}\".strip()\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        processed_text = preprocess_text(content)\n",
    "\n",
    "        processed_text = preprocess_text(content)\n",
    "        batch_texts.append(processed_text)\n",
    "        batch_idx.append(idx)\n",
    "\n",
    "        if len(batch_texts) >= batch_size:\n",
    "            # Process the batch\n",
    "            inputs = tokenizer(batch_texts, truncation=True, padding=True, return_tensors=\"pt\").to(embedding_model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = embedding_model(**inputs, output_hidden_states=True)\n",
    "                hidden_states = outputs.hidden_states[-1]\n",
    "                embeddings = hidden_states.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "            for i, doc_embedding in zip(batch_idx, embeddings):\n",
    "                doc = {\n",
    "                    col: row[col]\n",
    "                    for col in df.columns\n",
    "                    if col not in ['content', 'knowledge_embedding'] and not pd.isna(row[col])\n",
    "                }\n",
    "                doc['id'] = str(i)\n",
    "                doc['original_content'] = content\n",
    "                doc['processed_content'] = processed_text\n",
    "                doc['embedding'] = doc_embedding.tolist()\n",
    "\n",
    "                # Add label handling\n",
    "                if 'label' in doc:\n",
    "                    try:\n",
    "                        doc['label'] = int(doc['label'])\n",
    "                        if doc['label'] not in [0, 1]:\n",
    "                            continue\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                docs.append(doc)\n",
    "\n",
    "            # Clear the batch\n",
    "            batch_texts = []\n",
    "            batch_idx = []\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def store_in_mongodb(data, mongodb_uri, db_name, collection_name):\n",
    "    \"\"\"Store pre-processed data in MongoDB\"\"\"\n",
    "    client = MongoClient(mongodb_uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    if collection.count_documents({}) == 0:\n",
    "        print(f\"Inserting {len(data)} documents into `{collection_name}`...\")\n",
    "        for doc in data:\n",
    "            collection.update_one({\"id\": doc[\"id\"]}, {\"$set\": doc}, upsert=True)\n",
    "    else:\n",
    "        print(f\"Collection `{collection_name}` already contains data. Skipping insert.\")\n",
    "\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a1c3e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data from scratch...\n",
      "Loaded 23481 articles from fake.csv\n",
      "Loaded 21417 articles from true.csv\n",
      "Loaded 86087 articles from finefake\n",
      "Split data into 17133 training and 4284 test samples\n",
      "Processing document 0...\n",
      "Processing document 100...\n",
      "Processing document 200...\n",
      "Processing document 300...\n",
      "Processing document 400...\n",
      "Processing document 500...\n",
      "Processing document 600...\n",
      "Processing document 700...\n",
      "Processing document 800...\n",
      "Processing document 900...\n",
      "Processing document 1000...\n",
      "Processing document 1100...\n",
      "Processing document 1200...\n",
      "Processing document 1300...\n",
      "Processing document 1400...\n",
      "Processing document 1500...\n",
      "Processing document 1600...\n",
      "Processing document 1700...\n",
      "Processing document 1800...\n",
      "Processing document 1900...\n",
      "Processing document 2000...\n",
      "Processing document 2100...\n",
      "Processing document 2200...\n",
      "Processing document 2300...\n",
      "Processing document 2400...\n",
      "Processing document 2500...\n",
      "Processing document 2600...\n",
      "Processing document 2700...\n",
      "Processing document 2800...\n",
      "Processing document 2900...\n",
      "Processing document 3000...\n",
      "Processing document 3100...\n",
      "Processing document 3200...\n",
      "Processing document 3300...\n",
      "Processing document 3400...\n",
      "Processing document 3500...\n",
      "Processing document 3600...\n",
      "Processing document 3700...\n",
      "Processing document 3800...\n",
      "Processing document 3900...\n",
      "Processing document 4000...\n",
      "Processing document 4100...\n",
      "Processing document 4200...\n",
      "Processing document 4300...\n",
      "Processing document 4400...\n",
      "Processing document 4500...\n",
      "Processing document 4600...\n",
      "Processing document 4700...\n",
      "Processing document 4800...\n",
      "Processing document 4900...\n",
      "Processing document 5000...\n",
      "Processing document 5100...\n",
      "Processing document 5200...\n",
      "Processing document 5300...\n",
      "Processing document 5400...\n",
      "Processing document 5500...\n",
      "Processing document 5600...\n",
      "Processing document 5700...\n",
      "Processing document 5800...\n",
      "Processing document 5900...\n",
      "Processing document 6000...\n",
      "Processing document 6100...\n",
      "Processing document 6200...\n",
      "Processing document 6300...\n",
      "Processing document 6400...\n",
      "Processing document 6500...\n",
      "Processing document 6600...\n",
      "Processing document 6700...\n",
      "Processing document 6800...\n",
      "Processing document 6900...\n",
      "Processing document 7000...\n",
      "Processing document 7100...\n",
      "Processing document 7200...\n",
      "Processing document 7300...\n",
      "Processing document 7400...\n",
      "Processing document 7500...\n",
      "Processing document 7600...\n",
      "Processing document 7700...\n",
      "Processing document 7800...\n",
      "Processing document 7900...\n",
      "Processing document 8000...\n",
      "Processing document 8100...\n",
      "Processing document 8200...\n",
      "Processing document 8300...\n",
      "Processing document 8400...\n",
      "Processing document 8500...\n",
      "Processing document 8600...\n",
      "Processing document 8700...\n",
      "Processing document 8800...\n",
      "Processing document 8900...\n",
      "Processing document 9000...\n",
      "Processing document 9100...\n",
      "Processing document 9200...\n",
      "Processing document 9300...\n",
      "Processing document 9400...\n",
      "Processing document 9500...\n",
      "Processing document 9600...\n",
      "Processing document 9700...\n",
      "Processing document 9800...\n",
      "Processing document 9900...\n",
      "Processing document 10000...\n",
      "Processing document 10100...\n",
      "Processing document 10200...\n",
      "Processing document 10300...\n",
      "Processing document 10400...\n",
      "Processing document 10500...\n",
      "Processing document 10600...\n",
      "Processing document 10700...\n",
      "Processing document 10800...\n",
      "Processing document 10900...\n",
      "Processing document 11000...\n",
      "Processing document 11100...\n",
      "Processing document 11200...\n",
      "Processing document 11300...\n",
      "Processing document 11400...\n",
      "Processing document 11500...\n",
      "Processing document 11600...\n",
      "Processing document 11700...\n",
      "Processing document 11800...\n",
      "Processing document 11900...\n",
      "Processing document 12000...\n",
      "Processing document 12100...\n",
      "Processing document 12200...\n",
      "Processing document 12300...\n",
      "Processing document 12400...\n",
      "Processing document 12500...\n",
      "Processing document 12600...\n",
      "Processing document 12700...\n",
      "Processing document 12800...\n",
      "Processing document 12900...\n",
      "Processing document 13000...\n",
      "Processing document 13100...\n",
      "Processing document 13200...\n",
      "Processing document 13300...\n",
      "Processing document 13400...\n",
      "Processing document 13500...\n",
      "Processing document 13600...\n",
      "Processing document 13700...\n",
      "Processing document 13800...\n",
      "Processing document 13900...\n",
      "Processing document 14000...\n",
      "Processing document 14100...\n",
      "Processing document 14200...\n",
      "Processing document 14300...\n",
      "Processing document 14400...\n",
      "Processing document 14500...\n",
      "Processing document 14600...\n",
      "Processing document 14700...\n",
      "Processing document 14800...\n",
      "Processing document 14900...\n",
      "Processing document 15000...\n",
      "Processing document 15100...\n",
      "Processing document 15200...\n",
      "Processing document 15300...\n",
      "Processing document 15400...\n",
      "Processing document 15500...\n",
      "Processing document 15600...\n",
      "Processing document 15700...\n",
      "Processing document 15800...\n",
      "Processing document 15900...\n",
      "Processing document 16000...\n",
      "Processing document 16100...\n",
      "Processing document 16200...\n",
      "Processing document 16300...\n",
      "Processing document 16400...\n",
      "Processing document 16500...\n",
      "Processing document 16600...\n",
      "Processing document 16700...\n",
      "Processing document 16800...\n",
      "Processing document 16900...\n",
      "Processing document 17000...\n",
      "Processing document 17100...\n",
      "Processing document 17200...\n",
      "Processing document 17300...\n",
      "Processing document 17400...\n",
      "Processing document 17500...\n",
      "Processing document 17600...\n",
      "Processing document 17700...\n",
      "Processing document 17800...\n",
      "Processing document 17900...\n",
      "Processing document 18000...\n",
      "Processing document 18100...\n",
      "Processing document 18200...\n",
      "Processing document 18300...\n",
      "Processing document 18400...\n",
      "Processing document 18500...\n",
      "Processing document 18600...\n",
      "Processing document 18700...\n",
      "Processing document 18800...\n",
      "Processing document 18900...\n",
      "Processing document 19000...\n",
      "Processing document 19100...\n",
      "Processing document 19200...\n",
      "Processing document 19300...\n",
      "Processing document 19400...\n",
      "Processing document 19500...\n",
      "Processing document 19600...\n",
      "Processing document 19700...\n",
      "Processing document 19800...\n",
      "Processing document 19900...\n",
      "Processing document 20000...\n",
      "Processing document 20100...\n",
      "Processing document 20200...\n",
      "Processing document 20300...\n",
      "Processing document 20400...\n",
      "Processing document 20500...\n",
      "Processing document 20600...\n",
      "Processing document 20700...\n",
      "Processing document 20800...\n",
      "Processing document 20900...\n",
      "Processing document 21000...\n",
      "Processing document 21100...\n",
      "Processing document 21200...\n",
      "Processing document 21300...\n",
      "Processing document 21400...\n",
      "Collection `news_embeddings` already contains data. Skipping insert.\n",
      "Collection `news_embeddings_test` already contains data. Skipping insert.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17120, 4272)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_and_save_data(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23481 articles from fake.csv\n",
      "Loaded 21417 articles from true.csv\n",
      "Loaded 86087 articles from finefake\n",
      "Split data into 17133 training and 4284 test samples\n",
      "Processing training data...\n",
      "Generating embeddings for 17133 documents...\n",
      "Processing batch 1/343\n",
      "Connected to MongoDB successfully\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Process training data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m generate_and_store_embeddings(\n\u001b[0;32m      6\u001b[0m     df\u001b[38;5;241m=\u001b[39mtrain_df,\n\u001b[0;32m      7\u001b[0m     embedding_model\u001b[38;5;241m=\u001b[39membedding_model,\n\u001b[0;32m      8\u001b[0m     mongodb_uri\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mMONGODB_URI,\n\u001b[0;32m      9\u001b[0m     db_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mDB_NAME,\n\u001b[0;32m     10\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mCOLLECTION_NAME,\n\u001b[0;32m     11\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Store test data in a separate collection for evaluation\u001b[39;00m\n\u001b[0;32m     15\u001b[0m test_collection \u001b[38;5;241m=\u001b[39m setup_mongodb_collection(config\u001b[38;5;241m.\u001b[39mCOLLECTION_NAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 74\u001b[0m, in \u001b[0;36mgenerate_and_store_embeddings\u001b[1;34m(df, embedding_model, mongodb_uri, db_name, collection_name, batch_size)\u001b[0m\n\u001b[0;32m     71\u001b[0m             doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mencode(processed_text)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# Generate new embedding\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mencode(processed_text)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Store in MongoDB with retry logic\u001b[39;00m\n\u001b[0;32m     77\u001b[0m mongodb_operation(collection, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_one\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$set\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc}, upsert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:685\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    682\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 685\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    687\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:752\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m    750\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m    751\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[1;32m--> 752\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m    436\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    437\u001b[0m     key: value\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    440\u001b[0m }\n\u001b[1;32m--> 442\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    443\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1080\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1078\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m-> 1080\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1081\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1082\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1083\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1084\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1085\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1086\u001b[0m )\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1089\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:211\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    208\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(input_ids)\n\u001b[0;32m    212\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    214\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_and_split_data()\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "generate_and_store_embeddings(\n",
    "    df=train_df,\n",
    "    embedding_model=model,\n",
    "    mongodb_uri=config.MONGODB_URI,\n",
    "    db_name=config.DB_NAME,\n",
    "    collection_name=config.COLLECTION_NAME,\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "# Store test data in a separate collection for evaluation\n",
    "test_collection = setup_mongodb_collection(config.COLLECTION_NAME + \"_test\")\n",
    "print(\"Processing test data...\")\n",
    "generate_and_store_embeddings(\n",
    "    df=test_df,\n",
    "    embedding_model=embedding_model,\n",
    "    mongodb_uri=config.MONGODB_URI,\n",
    "    db_name=config.DB_NAME,\n",
    "    collection_name=config.COLLECTION_NAME,\n",
    "    batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e7b7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system_with_tokenizer(classifier, tokenizer, mongodb_uri, db_name, test_collection_name):\n",
    "    \"\"\"Evaluate the system on test data with proper tokenization for long texts\"\"\"\n",
    "    # Connect to the test collection\n",
    "    client = MongoClient(mongodb_uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[test_collection_name]\n",
    "    \n",
    "    # Retrieve all test documents\n",
    "    test_docs = list(collection.find({}))\n",
    "    \n",
    "    # Initialize metrics\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    print(f\"Evaluating on {len(test_docs)} test documents...\")\n",
    "    \n",
    "    for doc in test_docs:\n",
    "        try:\n",
    "            # Get the true label\n",
    "            if 'label' in doc:\n",
    "                true_label = doc['label']\n",
    "                \n",
    "                true_label = int(true_label)  # in case it's a string like '1'\n",
    "                assert true_label in [0, 1], f\"Invalid label: {true_label}\"\n",
    "                true_labels.append(true_label)\n",
    "                \n",
    "                \n",
    "                # Get text for classification\n",
    "                text = doc.get('processed_content', doc.get('original_content', ''))\n",
    "                \n",
    "                # Properly truncate using the model's tokenizer\n",
    "                encoded_input = tokenizer(text, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "                # Move tokenized input to same device as the model\n",
    "                device = next(classifier.model.parameters()).device\n",
    "                encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "                \n",
    "                # Run classifier with tokenized input\n",
    "                with torch.no_grad():\n",
    "                    output = classifier.model(**encoded_input)\n",
    "\n",
    "                    logits = output.logits\n",
    "                    \n",
    "                    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "                \n",
    "                # Map the output to your label scheme\n",
    "                pred_label = predicted_class  # Adjust as needed for your model\n",
    "                \n",
    "                predicted_labels.append(pred_label)\n",
    "                \n",
    "                # Store prediction in MongoDB for later analysis\n",
    "                collection.update_one(\n",
    "                    {\"_id\": doc[\"_id\"]},\n",
    "                    {\"$set\": {\"predicted_label\": pred_label}}\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                print(f\"Document {doc.get('id', 'unknown')} missing label, skipping...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating document {doc.get('id', 'unknown')}: {e}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if true_labels and predicted_labels:\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        precision = precision_score(true_labels, predicted_labels)\n",
    "        recall = recall_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels)\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        \n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Calculate AUC if possible\n",
    "        try:\n",
    "            auc = roc_auc_score(true_labels, predicted_labels)\n",
    "            print(f\"AUC: {auc:.4f}\")\n",
    "        except:\n",
    "            print(\"Could not calculate AUC\")\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"confusion_matrix\": cm.tolist()\n",
    "        }\n",
    "    else:\n",
    "        print(\"No valid predictions to evaluate\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddeaa624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling model...\n",
      "Success! Logits: tensor([[0.1382, 0.0289]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your model\n",
    "model = classifier.model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Choose a test string (you can pull one from MongoDB if needed)\n",
    "text = \"This is a test article. It talks about some fake event.\"\n",
    "\n",
    "# Tokenize and move to GPU\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Calling model...\")\n",
    "    outputs = model(**inputs)\n",
    "    print(\"Success! Logits:\", outputs.logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb19426",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d0aa6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 4284 test documents...\n",
      "\n",
      "Evaluation Results:\n",
      "Accuracy: 0.7003\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3000 1284]\n",
      " [   0    0]]\n",
      "Could not calculate AUC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_results = evaluate_system_with_tokenizer(\n",
    "    classifier=classifier,\n",
    "    tokenizer=tokenizer,\n",
    "    mongodb_uri=config.MONGODB_URI,\n",
    "    db_name=config.DB_NAME,\n",
    "    test_collection_name=config.COLLECTION_NAME + \"_test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb55d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_documents(query_text, embedding_model, collection, top_k=3):\n",
    "    \"\"\"Search for similar documents using vector similarity\"\"\"\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedding_model.encode(preprocess_text(query_text))\n",
    "    \n",
    "    # Search in MongoDB using vector similarity\n",
    "    results = collection.aggregate([\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"queryVector\": query_embedding.tolist(),\n",
    "                \"numCandidates\": top_k * 3,  # Request more candidates for better results\n",
    "                \"limit\": top_k\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"title\": 1,\n",
    "                \"text\": 1,\n",
    "                \"subject\": 1, \n",
    "                \"date\": 1,\n",
    "                \"score\": {\"$meta\": \"searchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    return list(results)\n",
    "\n",
    "def process_query_with_rag(query, embedding_model, collection, qa_pipeline):\n",
    "    \"\"\"Process user query with RAG approach\"\"\"\n",
    "    # Find relevant documents\n",
    "    similar_docs = search_similar_documents(query, embedding_model, collection)\n",
    "    \n",
    "    # If no relevant documents found\n",
    "    if not similar_docs:\n",
    "        prompt = f\"Question: {query}\\nAnswer: Based on my knowledge,\"\n",
    "        return qa_pipeline(prompt)[0][\"generated_text\"]\n",
    "    \n",
    "    # Build context from similar documents\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(similar_docs):\n",
    "        context += f\"Document {i+1}:\\nTitle: {doc['title']}\\nContent: {doc['text']}\\nSubject: {doc.get('subject', 'N/A')}\\nDate: {doc.get('date', 'N/A')}\\n\\n\"\n",
    "    \n",
    "    # Build RAG prompt\n",
    "    prompt = f\"\"\"Context information is below.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the question: {query}\n",
    "If the context doesn't contain enough information to answer the question, say so.\n",
    "Check if the news might be fake by analyzing inconsistencies, extreme language, or unverifiable claims.\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = qa_pipeline(prompt, max_length=512, num_return_sequences=1)[0][\"generated_text\"]\n",
    "    \n",
    "    # Extract only the generated answer part (after the prompt)\n",
    "    answer = response[len(prompt):]\n",
    "    return answer.strip()\n",
    "\n",
    "def detect_fake_news(news_text, qa_pipeline):\n",
    "    \"\"\"Specialized function to analyze if a news item might be fake\"\"\"\n",
    "    prompt = f\"\"\"Analyze the following news text and determine if it might be fake news.\n",
    "Consider these aspects:\n",
    "1. Is the language extremely emotional or sensationalist?\n",
    "2. Are there unverifiable claims or statistics?\n",
    "3. Does it contain inconsistencies or logical fallacies?\n",
    "4. Is there extreme political bias?\n",
    "5. Does it use manipulative tactics?\n",
    "\n",
    "News text:\n",
    "{news_text}\n",
    "\n",
    "Provide a detailed analysis with evidence from the text, and conclude with a judgment on whether this is likely real news or fake news.\n",
    "\"\"\"\n",
    "    \n",
    "    response = qa_pipeline(prompt, max_length=768, num_return_sequences=1)[0][\"generated_text\"]\n",
    "    # Extract only the generated answer part\n",
    "    answer = response[len(prompt):]\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query):\n",
    "    \"\"\"Process a question using the RAG system\"\"\"\n",
    "    result = process_query_with_rag(query, embedding_model, collection, qa_pipeline)\n",
    "    print(f\"Question: {query}\")\n",
    "    print(\"\\nRAG Response:\")\n",
    "    print(result)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "def analyze_news(news_text):\n",
    "    \"\"\"Analyze if a news article might be fake\"\"\"\n",
    "    result = detect_fake_news(news_text, qa_pipeline)\n",
    "    print(\"Fake News Analysis:\")\n",
    "    print(result)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Example usage:\n",
    "# ask_question(\"What are common characteristics of fake news about climate change?\")\n",
    "\n",
    "# sample_news = \"\"\"BREAKING: Scientists Found that Climate Change is a Hoax Created by China to Decrease US Manufacturing. \n",
    "# Documents leaked by anonymous sources show that 97% of scientists were paid to promote this theory.\"\"\"\n",
    "# analyze_news(sample_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query):\n",
    "    \"\"\"Process a question using the RAG system\"\"\"\n",
    "    result = process_query_with_rag(query, embedding_model, collection, qa_pipeline)\n",
    "    print(f\"Question: {query}\")\n",
    "    print(\"\\nRAG Response:\")\n",
    "    print(result)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "def analyze_news(news_text):\n",
    "    \"\"\"Analyze if a news article might be fake\"\"\"\n",
    "    result = detect_fake_news(news_text, qa_pipeline)\n",
    "    print(\"Fake News Analysis:\")\n",
    "    print(result)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Example usage:\n",
    "# ask_question(\"What are common characteristics of fake news about climate change?\")\n",
    "\n",
    "# sample_news = \"\"\"BREAKING: Scientists Found that Climate Change is a Hoax Created by China to Decrease US Manufacturing. \n",
    "# Documents leaked by anonymous sources show that 97% of scientists were paid to promote this theory.\"\"\"\n",
    "# analyze_news(sample_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c83913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_mode():\n",
    "    print(\"\\nFake News Detection RAG System Ready!\")\n",
    "    print(\"Enter 'q' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nEnter your question or paste news to analyze: \")\n",
    "        if query.lower() == 'q':\n",
    "            break\n",
    "            \n",
    "        print(\"\\nProcessing...\")\n",
    "        \n",
    "        # Detect if this is a query or news text to analyze (simple heuristic)\n",
    "        if len(query.split()) > 30:  # Longer text is likely news to analyze\n",
    "            result = detect_fake_news(query, qa_pipeline)\n",
    "            print(\"\\nFake News Analysis:\")\n",
    "        else:  # Shorter text is likely a question\n",
    "            result = process_query_with_rag(query, embedding_model, collection, qa_pipeline)\n",
    "            print(\"\\nRAG Response:\")\n",
    "            \n",
    "        print(result)\n",
    "\n",
    "# Run this cell to start interactive mode\n",
    "# interactive_mode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
