{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# -----------------------\n",
    "# Optional: Ensure punkt is downloaded (if you want fallback to word_tokenize later)\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# -----------------------\n",
    "# Load tokenizer\n",
    "tokenizer = TreebankWordTokenizer().tokenize\n",
    "\n",
    "# -----------------------\n",
    "# Dataset loading function\n",
    "def get_hasib18_fns(*, include_instruction=False):\n",
    "    prefix = \"Instruction: Classify the following news article as real or fake.\\n\\nInput: \"\n",
    "    suffix = \"\\n\\nOutput: fake\"\n",
    "    l_pre = len(prefix)\n",
    "    l_suf = len(suffix)\n",
    "\n",
    "    ds = load_dataset(\"Hasib18/fake-news-dataset\")\n",
    "    train_df = ds[\"train\"].to_pandas()\n",
    "    test_df = ds[\"test\"].to_pandas()\n",
    "    if not include_instruction:\n",
    "        train_df[\"text\"] = train_df[\"text\"].apply(lambda x: x[l_pre:-l_suf])\n",
    "        test_df[\"text\"] = test_df[\"text\"].apply(lambda x: x[l_pre:-l_suf])\n",
    "    return train_df, test_df\n",
    "\n",
    "# -----------------------\n",
    "# Custom Dataset class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = tokenizer(self.texts[idx])[:self.max_len]\n",
    "        ids = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "        padding = [self.vocab[\"<pad>\"]] * (self.max_len - len(ids))\n",
    "        ids = ids + padding\n",
    "        return torch.tensor(ids), torch.tensor(self.labels[idx])\n",
    "\n",
    "# -----------------------\n",
    "# Vocab builder\n",
    "def build_vocab(token_lists, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in token_lists:\n",
    "        counter.update(tokens)\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for token, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# -----------------------\n",
    "# RNN model\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        out = self.fc(hidden.squeeze(0))\n",
    "        return out\n",
    "\n",
    "# -----------------------\n",
    "# Training function\n",
    "def train_rnn(total_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"üì¶ Using device: {device}\")\n",
    "\n",
    "    train_df, test_df = get_hasib18_fns(include_instruction=False)\n",
    "\n",
    "    # Tokenize all texts\n",
    "    if os.path.exists(\"train_tokens.pkl\") and os.path.exists(\"test_tokens.pkl\"):\n",
    "        with open(\"train_tokens.pkl\", \"rb\") as f:\n",
    "            train_tokens = pickle.load(f)\n",
    "        with open(\"test_tokens.pkl\", \"rb\") as f:\n",
    "            test_tokens = pickle.load(f)\n",
    "        print(\"‚úÖ Loaded cached tokenized data.\")\n",
    "    else:\n",
    "        train_tokens = [tokenizer(text) for text in train_df[\"text\"]]\n",
    "        test_tokens = [tokenizer(text) for text in test_df[\"text\"]]\n",
    "        with open(\"train_tokens.pkl\", \"wb\") as f:\n",
    "            pickle.dump(train_tokens, f)\n",
    "        with open(\"test_tokens.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_tokens, f)\n",
    "        print(\"‚úÖ Tokenized data and saved to disk.\")\n",
    "\n",
    "    # Build vocab\n",
    "    vocab = build_vocab(train_tokens)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = NewsDataset(train_df[\"text\"].tolist(), train_df[\"label\"].tolist(), vocab, max_len=50)\n",
    "    test_dataset = NewsDataset(test_df[\"text\"].tolist(), test_df[\"label\"].tolist(), vocab, max_len=50)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    model = RNNClassifier(len(vocab)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    checkpoint_path = \"checkpoint.pt\"\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"üîÅ Found checkpoint! Resuming training...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"‚úÖ Resuming from epoch {start_epoch}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{total_epochs}\", leave=False)\n",
    "        for x_batch, y_batch in loop:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_batch)\n",
    "            loss = criterion(out, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(out, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Training metrics\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        print(f\"\\nüü© Epoch {epoch+1} Results:\")\n",
    "        print(f\"Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        print(f\"Train Acc: {train_acc:.4f} | Precision: {train_precision:.4f} | Recall: {train_recall:.4f} | F1: {train_f1:.4f}\")\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_preds, test_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                out = model(x_batch)\n",
    "                preds = torch.argmax(out, dim=1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "                test_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        test_acc = accuracy_score(test_labels, test_preds)\n",
    "        test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='weighted')\n",
    "        print(f\"üß™ Test Acc: {test_acc:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f} | F1: {test_f1:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, checkpoint_path)\n",
    "        print(f\"üíæ Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "# -----------------------\n",
    "# Run everything\n",
    "if __name__ == \"__main__\":\n",
    "    train_rnn(total_epochs=30)  # Or however many total epochs you want\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
