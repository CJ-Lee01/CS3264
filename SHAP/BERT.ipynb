{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35134fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from shap.maskers import Text\n",
    "\n",
    "# === Setup NLTK ===\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "try:\n",
    "    nltk.download('punkt_tab', download_dir=nltk_data_path)\n",
    "except:\n",
    "    nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# === Device setup ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# === Load and clean dataset ===\n",
    "def get_cleaned_dataset(include_instruction=False):\n",
    "    prefix = \"Instruction: Classify the following news article as real or fake.\\n\\nInput: \"\n",
    "    suffix = \"\\n\\nOutput: fake\"\n",
    "    l_pre, l_suf = len(prefix), len(suffix)\n",
    "    dataset = load_dataset(\"Hasib18/fake-news-dataset\")\n",
    "    for split in dataset:\n",
    "        if not include_instruction:\n",
    "            dataset[split] = dataset[split].map(lambda x: {\"text\": x[\"text\"][l_pre:-l_suf]})\n",
    "    return dataset\n",
    "\n",
    "dataset = get_cleaned_dataset()\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# === LoRA-wrapped DistilBERT ===\n",
    "base_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# === Training config ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=1000,\n",
    "    report_to=\"none\",\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# === Train and save ===\n",
    "print(\"\\nüöÄ Training LoRA-augmented DistilBERT...\")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./trained_distilbert_lora\")\n",
    "\n",
    "# === Evaluation ===\n",
    "preds_output = trainer.predict(tokenized_dataset[\"test\"])\n",
    "preds = np.argmax(preds_output.predictions, axis=1)\n",
    "labels = preds_output.label_ids\n",
    "print(f\"\\nüéØ F1 Score: {f1_score(labels, preds):.4f}\")\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(labels, preds, target_names=[\"FAKE\", \"REAL\"]))\n",
    "\n",
    "# === SHAP wrapper ===\n",
    "def wrapped_model(texts):\n",
    "    if isinstance(texts, (str, np.generic)) or not isinstance(texts, list):\n",
    "        texts = [str(t) for t in np.atleast_1d(texts)]\n",
    "    encodings = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encodings).logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# === SHAP Sentence Explanation ===\n",
    "article = \"\"\"\n",
    "Trump claimed the election was stolen and widespread fraud occurred.\n",
    "Officials from both parties denied these claims and upheld the results.\n",
    "An independent audit found no evidence of tampering.\n",
    "Social media platforms flagged the original post for misinformation.\n",
    "\"\"\"\n",
    "sentences = sent_tokenize(article)\n",
    "\n",
    "print(\"\\nüìù Sentences:\")\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"{i+1}. {s}\")\n",
    "\n",
    "print(\"\\nüîç Explaining with SHAP (terminal version)...\")\n",
    "text_masker = Text(tokenizer)\n",
    "explainer = shap.Explainer(wrapped_model, text_masker)\n",
    "shap_values = explainer(sentences)\n",
    "\n",
    "# Predicted class\n",
    "pred_class = np.argmax(np.mean(wrapped_model(sentences), axis=0))\n",
    "class_name = ['FAKE', 'REAL'][pred_class]\n",
    "print(f\"\\nüìä Predicted class: {class_name}\")\n",
    "\n",
    "# ‚úÖ FINAL FIX FOR FORMAT ERROR\n",
    "contributions = shap_values.values\n",
    "\n",
    "print(f\"\\nüßæ Sentence-level SHAP values for class: {class_name}\")\n",
    "print(f\"üîç shap_values.values shape: {contributions.shape}\")\n",
    "print(f\"üîç type of shap_values.values: {type(contributions)}\")\n",
    "\n",
    "for i, (sentence, value) in enumerate(zip(sentences, contributions)):\n",
    "    print(f\"\\n{i+1}. {sentence}\")\n",
    "    print(f\"   Raw value shape: {value.shape}\")\n",
    "    try:\n",
    "        scalar = float(value[:, pred_class].sum())  # Aggregate token contributions for predicted class\n",
    "        print(f\"   ‚úÖ Contribution to {class_name}: {scalar:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
